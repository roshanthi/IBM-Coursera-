{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.load('X_train_Gray.npy')\n",
    "Y_train = np.load('Y_train_Gray.npy')\n",
    "X_test = np.load('X_test_Gray.npy')\n",
    "Y_test = np.load('Y_test_Gray.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17636 samples, validate on 4410 samples\n",
      "Epoch 1/20\n",
      "17636/17636 [==============================] - 405s 23ms/step - loss: 0.6020 - acc: 0.6697 - val_loss: 0.4839 - val_acc: 0.7683\n",
      "Epoch 2/20\n",
      "17636/17636 [==============================] - 398s 23ms/step - loss: 0.4445 - acc: 0.7945 - val_loss: 0.4142 - val_acc: 0.8209\n",
      "Epoch 3/20\n",
      "17636/17636 [==============================] - 396s 22ms/step - loss: 0.3300 - acc: 0.8594 - val_loss: 0.3662 - val_acc: 0.8654\n",
      "Epoch 4/20\n",
      "17636/17636 [==============================] - 396s 22ms/step - loss: 0.2501 - acc: 0.8979 - val_loss: 0.2196 - val_acc: 0.9124\n",
      "Epoch 5/20\n",
      "17636/17636 [==============================] - 386s 22ms/step - loss: 0.2078 - acc: 0.9187 - val_loss: 0.2199 - val_acc: 0.9141\n",
      "Epoch 6/20\n",
      "17636/17636 [==============================] - 388s 22ms/step - loss: 0.1850 - acc: 0.9285 - val_loss: 0.1765 - val_acc: 0.9355\n",
      "Epoch 7/20\n",
      "17636/17636 [==============================] - 380s 22ms/step - loss: 0.1716 - acc: 0.9361 - val_loss: 0.1643 - val_acc: 0.9406\n",
      "Epoch 8/20\n",
      "17636/17636 [==============================] - 388s 22ms/step - loss: 0.1641 - acc: 0.9383 - val_loss: 0.1768 - val_acc: 0.9288\n",
      "Epoch 9/20\n",
      "17636/17636 [==============================] - 388s 22ms/step - loss: 0.1552 - acc: 0.9432 - val_loss: 0.1945 - val_acc: 0.9217\n",
      "Epoch 10/20\n",
      "17636/17636 [==============================] - 390s 22ms/step - loss: 0.1448 - acc: 0.9475 - val_loss: 0.1689 - val_acc: 0.9407\n",
      "Epoch 11/20\n",
      "17636/17636 [==============================] - 391s 22ms/step - loss: 0.1371 - acc: 0.9500 - val_loss: 0.1474 - val_acc: 0.9468\n",
      "Epoch 12/20\n",
      "17636/17636 [==============================] - 392s 22ms/step - loss: 0.1335 - acc: 0.9503 - val_loss: 0.1451 - val_acc: 0.9490\n",
      "Epoch 13/20\n",
      "17636/17636 [==============================] - 390s 22ms/step - loss: 0.1296 - acc: 0.9509 - val_loss: 0.1428 - val_acc: 0.9463\n",
      "Epoch 14/20\n",
      "17636/17636 [==============================] - 393s 22ms/step - loss: 0.1273 - acc: 0.9540 - val_loss: 0.1450 - val_acc: 0.9446\n",
      "Epoch 15/20\n",
      "17636/17636 [==============================] - 391s 22ms/step - loss: 0.1199 - acc: 0.9551 - val_loss: 0.1403 - val_acc: 0.9466\n",
      "Epoch 16/20\n",
      "17636/17636 [==============================] - 391s 22ms/step - loss: 0.1139 - acc: 0.9581 - val_loss: 0.1444 - val_acc: 0.9478\n",
      "Epoch 17/20\n",
      "17636/17636 [==============================] - 383s 22ms/step - loss: 0.1113 - acc: 0.9579 - val_loss: 0.1543 - val_acc: 0.9466\n",
      "Epoch 18/20\n",
      "17636/17636 [==============================] - 387s 22ms/step - loss: 0.1080 - acc: 0.9598 - val_loss: 0.1407 - val_acc: 0.9483\n",
      "Epoch 19/20\n",
      "17636/17636 [==============================] - 392s 22ms/step - loss: 0.1026 - acc: 0.9606 - val_loss: 0.1477 - val_acc: 0.9475\n",
      "Epoch 20/20\n",
      "17636/17636 [==============================] - 391s 22ms/step - loss: 0.0971 - acc: 0.9651 - val_loss: 0.1508 - val_acc: 0.9488\n",
      "Test loss: 0.158624228293\n",
      "Test Accuracy 0.946298984035\n"
     ]
    }
   ],
   "source": [
    "#Building Deep Convolution Neural Network with Hisogram Equalized image set\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "batch_size=64\n",
    "epochs=20\n",
    "Img_size=100\n",
    "num_classes=2\n",
    "input_shape=(Img_size, Img_size, 1)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3),             #1st layer - Convolution layer with 32 neurons and 3 x 3 matrix to scan the image\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))   \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))            #2ndlayer - Maxpooling layer with 2 x 2 matrix to scan\n",
    "model.add(Dropout(0.25))                            #to avoid overfitting dropping 25% of neurons in each iteration\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))       #3rd layer - Convolution layer with 64 neurons 3 x3 matrix to scan the image\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))            #4th layer - Maxpooling layer with 2 x 2 matrix to scan\n",
    "model.add(Dropout(0.25))                            #to avoid overfitting dropping 25% of neurons in each iteration\n",
    "\n",
    "model.add(Conv2D(128,(3,3),activation='relu'))      #5th layer - Convolution layer with 128 neurons 3 x3 matrix to scan the image\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))            #6th layer - Maxpooling layer with 2 x 2 matrix to scan\n",
    "model.add(Dropout(0.25))                            #to avoid overfitting dropping 25% of neurons in each iteration\n",
    "\n",
    "model.add(Conv2D(256,(3,3),activation='relu'))      #7th layer - Convolution layer with 256 neurons 3 x3 matrix to scan the image\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))            #8th layer - Maxpooling layer with 2 x 2 matrix to scan\n",
    "model.add(Dropout(0.25))                            #to avoid overfitting dropping 25% of neurons in each iteration\n",
    "\n",
    "model.add(Flatten())                        \n",
    "\n",
    "model.add(Dense(512, activation='relu'))            #9th layer - Fully connected Dense layer with 512 neurons\n",
    "model.add(Dropout(0.5))                             #to avoid overfitting dropping 50% of the neurons in each iteration\n",
    "model.add(Dense(num_classes, activation='sigmoid')) #output layer\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_split=0.20)\n",
    "score=model.evaluate(X_test,Y_test,verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test Accuracy', score[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
