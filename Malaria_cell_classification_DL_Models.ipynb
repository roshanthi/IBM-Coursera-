{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.load('X_train_color.npy')\n",
    "Y_train = np.load('Y_train_color.npy')\n",
    "X_test = np.load('X_test_color.npy')\n",
    "Y_test = np.load('Y_test_color.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17636 samples, validate on 4410 samples\n",
      "Epoch 1/20\n",
      "17636/17636 [==============================] - 409s 23ms/step - loss: 0.6567 - acc: 0.6174 - val_loss: 0.3387 - val_acc: 0.8655\n",
      "Epoch 2/20\n",
      "17636/17636 [==============================] - 409s 23ms/step - loss: 0.2306 - acc: 0.9180 - val_loss: 0.1935 - val_acc: 0.9381\n",
      "Epoch 3/20\n",
      "17636/17636 [==============================] - 408s 23ms/step - loss: 0.1687 - acc: 0.9483 - val_loss: 0.1467 - val_acc: 0.9537\n",
      "Epoch 4/20\n",
      "17636/17636 [==============================] - 411s 23ms/step - loss: 0.1586 - acc: 0.9527 - val_loss: 0.1482 - val_acc: 0.9531\n",
      "Epoch 5/20\n",
      "17636/17636 [==============================] - 405s 23ms/step - loss: 0.1523 - acc: 0.9541 - val_loss: 0.1362 - val_acc: 0.9575\n",
      "Epoch 6/20\n",
      "17636/17636 [==============================] - 409s 23ms/step - loss: 0.1459 - acc: 0.9556 - val_loss: 0.1372 - val_acc: 0.9551\n",
      "Epoch 7/20\n",
      "17636/17636 [==============================] - 406s 23ms/step - loss: 0.1424 - acc: 0.9557 - val_loss: 0.1344 - val_acc: 0.9583\n",
      "Epoch 8/20\n",
      "17636/17636 [==============================] - 413s 23ms/step - loss: 0.1376 - acc: 0.9569 - val_loss: 0.1357 - val_acc: 0.9571\n",
      "Epoch 9/20\n",
      "17636/17636 [==============================] - 411s 23ms/step - loss: 0.1368 - acc: 0.9564 - val_loss: 0.1338 - val_acc: 0.9585\n",
      "Epoch 10/20\n",
      "17636/17636 [==============================] - 412s 23ms/step - loss: 0.1350 - acc: 0.9580 - val_loss: 0.1430 - val_acc: 0.9531\n",
      "Epoch 11/20\n",
      "17636/17636 [==============================] - 407s 23ms/step - loss: 0.1260 - acc: 0.9602 - val_loss: 0.1262 - val_acc: 0.9570\n",
      "Epoch 12/20\n",
      "17636/17636 [==============================] - 399s 23ms/step - loss: 0.1252 - acc: 0.9600 - val_loss: 0.1361 - val_acc: 0.9579\n",
      "Epoch 13/20\n",
      "17636/17636 [==============================] - 412s 23ms/step - loss: 0.1224 - acc: 0.9601 - val_loss: 0.1465 - val_acc: 0.9519\n",
      "Epoch 14/20\n",
      "17636/17636 [==============================] - 411s 23ms/step - loss: 0.1196 - acc: 0.9611 - val_loss: 0.1208 - val_acc: 0.9576\n",
      "Epoch 15/20\n",
      "17636/17636 [==============================] - 410s 23ms/step - loss: 0.1206 - acc: 0.9604 - val_loss: 0.1369 - val_acc: 0.9516\n",
      "Epoch 16/20\n",
      "17636/17636 [==============================] - 415s 24ms/step - loss: 0.1138 - acc: 0.9615 - val_loss: 0.1258 - val_acc: 0.9577\n",
      "Epoch 17/20\n",
      "17636/17636 [==============================] - 411s 23ms/step - loss: 0.1125 - acc: 0.9623 - val_loss: 0.1313 - val_acc: 0.9533\n",
      "Epoch 18/20\n",
      "17636/17636 [==============================] - 410s 23ms/step - loss: 0.1104 - acc: 0.9632 - val_loss: 0.1204 - val_acc: 0.9612\n",
      "Epoch 19/20\n",
      "17636/17636 [==============================] - 416s 24ms/step - loss: 0.1071 - acc: 0.9636 - val_loss: 0.1250 - val_acc: 0.9574\n",
      "Epoch 20/20\n",
      "17636/17636 [==============================] - 418s 24ms/step - loss: 0.1031 - acc: 0.9648 - val_loss: 0.1241 - val_acc: 0.9585\n",
      "Test loss: 0.139859490161\n",
      "Test Accuracy 0.958091436865\n"
     ]
    }
   ],
   "source": [
    "#Building the 1st Deep Convolution Neural Network\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "batch_size=64\n",
    "epochs=20\n",
    "Img_size=100\n",
    "num_classes=2\n",
    "input_shape=(Img_size, Img_size, 3)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3),             #1st layer - Convolution layer with 32 neurons and 3 x 3 matrix to scan the image\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))   \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))            #2ndlayer - Maxpooling layer with 2 x 2 matrix to scan\n",
    "model.add(Dropout(0.25))                            #to avoid overfitting dropping 25% of neurons in each iteration\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))       #3rd layer - Convolution layer with 64 neurons 3 x3 matrix to scan the image\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))            #4th layer - Maxpooling layer with 2 x 2 matrix to scan\n",
    "model.add(Dropout(0.25))                            #to avoid overfitting dropping 25% of neurons in each iteration\n",
    "\n",
    "model.add(Conv2D(128,(3,3),activation='relu'))      #5th layer - Convolution layer with 128 neurons 3 x3 matrix to scan the image\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))            #6th layer - Maxpooling layer with 2 x 2 matrix to scan\n",
    "model.add(Dropout(0.25))                            #to avoid overfitting dropping 25% of neurons in each iteration\n",
    "\n",
    "model.add(Conv2D(256,(3,3),activation='relu'))      #7th layer - Convolution layer with 256 neurons 3 x3 matrix to scan the image\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))            #8th layer - Maxpooling layer with 2 x 2 matrix to scan\n",
    "model.add(Dropout(0.25))                            #to avoid overfitting dropping 25% of neurons in each iteration\n",
    "\n",
    "model.add(Flatten())                        \n",
    "\n",
    "model.add(Dense(512, activation='relu'))            #9 th Layer Fully connected Dense layer with 512 neurons\n",
    "model.add(Dropout(0.5))                             #to avoid overfitting dropping 50% of the neurons in each iteration\n",
    "model.add(Dense(num_classes, activation='sigmoid')) #output layer\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_split=0.20)\n",
    "score=model.evaluate(X_test,Y_test,verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test Accuracy', score[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17636 samples, validate on 4410 samples\n",
      "Epoch 1/20\n",
      "17636/17636 [==============================] - 412s 23ms/step - loss: 0.6906 - acc: 0.5385 - val_loss: 0.6783 - val_acc: 0.5279\n",
      "Epoch 2/20\n",
      "17636/17636 [==============================] - 414s 23ms/step - loss: 0.4405 - acc: 0.7915 - val_loss: 0.1777 - val_acc: 0.9424\n",
      "Epoch 3/20\n",
      "17636/17636 [==============================] - 412s 23ms/step - loss: 0.1794 - acc: 0.9462 - val_loss: 0.1541 - val_acc: 0.9563\n",
      "Epoch 4/20\n",
      "17636/17636 [==============================] - 410s 23ms/step - loss: 0.1588 - acc: 0.9551 - val_loss: 0.1487 - val_acc: 0.9545\n",
      "Epoch 5/20\n",
      "17636/17636 [==============================] - 410s 23ms/step - loss: 0.1541 - acc: 0.9555 - val_loss: 0.1352 - val_acc: 0.9565\n",
      "Epoch 6/20\n",
      "17636/17636 [==============================] - 414s 23ms/step - loss: 0.1495 - acc: 0.9555 - val_loss: 0.1383 - val_acc: 0.9587\n",
      "Epoch 7/20\n",
      "17636/17636 [==============================] - 402s 23ms/step - loss: 0.1438 - acc: 0.9580 - val_loss: 0.1363 - val_acc: 0.9559\n",
      "Epoch 8/20\n",
      "17636/17636 [==============================] - 399s 23ms/step - loss: 0.1451 - acc: 0.9566 - val_loss: 0.1285 - val_acc: 0.9590\n",
      "Epoch 9/20\n",
      "17636/17636 [==============================] - 410s 23ms/step - loss: 0.1412 - acc: 0.9575 - val_loss: 0.1301 - val_acc: 0.9593\n",
      "Epoch 10/20\n",
      "17636/17636 [==============================] - 411s 23ms/step - loss: 0.1408 - acc: 0.9553 - val_loss: 0.1253 - val_acc: 0.9599\n",
      "Epoch 11/20\n",
      "17636/17636 [==============================] - 412s 23ms/step - loss: 0.1390 - acc: 0.9571 - val_loss: 0.1257 - val_acc: 0.9583\n",
      "Epoch 12/20\n",
      "17636/17636 [==============================] - 413s 23ms/step - loss: 0.1381 - acc: 0.9575 - val_loss: 0.1251 - val_acc: 0.9580\n",
      "Epoch 13/20\n",
      "17636/17636 [==============================] - 410s 23ms/step - loss: 0.1329 - acc: 0.9586 - val_loss: 0.1283 - val_acc: 0.9578\n",
      "Epoch 14/20\n",
      "17636/17636 [==============================] - 410s 23ms/step - loss: 0.1348 - acc: 0.9581 - val_loss: 0.1236 - val_acc: 0.9592\n",
      "Epoch 15/20\n",
      "17636/17636 [==============================] - 413s 23ms/step - loss: 0.1329 - acc: 0.9582 - val_loss: 0.1315 - val_acc: 0.9567\n",
      "Epoch 16/20\n",
      "17636/17636 [==============================] - 413s 23ms/step - loss: 0.1285 - acc: 0.9594 - val_loss: 0.1312 - val_acc: 0.9568\n",
      "Epoch 17/20\n",
      "17636/17636 [==============================] - 411s 23ms/step - loss: 0.1292 - acc: 0.9598 - val_loss: 0.1191 - val_acc: 0.9583\n",
      "Epoch 18/20\n",
      "17636/17636 [==============================] - 402s 23ms/step - loss: 0.1285 - acc: 0.9585 - val_loss: 0.1310 - val_acc: 0.9535\n",
      "Epoch 19/20\n",
      "17636/17636 [==============================] - 401s 23ms/step - loss: 0.1297 - acc: 0.9591 - val_loss: 0.1173 - val_acc: 0.9602\n",
      "Epoch 20/20\n",
      "17636/17636 [==============================] - 411s 23ms/step - loss: 0.1224 - acc: 0.9623 - val_loss: 0.1223 - val_acc: 0.9603\n",
      "Test loss: 0.137497663376\n",
      "Test Accuracy 0.956912191582\n"
     ]
    }
   ],
   "source": [
    "#Building the 2nd Deep Convolution Neural Network\n",
    "\n",
    "#Modifications - adding more dense layers, changing epochs to 20 (training longer)\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "batch_size=64\n",
    "epochs=20\n",
    "Img_size=100\n",
    "num_classes=2\n",
    "input_shape=(Img_size, Img_size, 3)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3),             #1st layer - Convolution layer with 32 neurons and 3 x 3 matrix to scan the image\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))   \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))            #2ndlayer - Maxpooling layer with 2 x 2 matrix to scan\n",
    "model.add(Dropout(0.25))                            #to avoid overfitting dropping 25% of neurons in each iteration\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))       #3rd layer - Convolution layer with 64 neurons 3 x3 matrix to scan the image\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))            #4th layer - Maxpooling layer with 2 x 2 matrix to scan\n",
    "model.add(Dropout(0.25))                            #to avoid overfitting dropping 25% of neurons in each iteration\n",
    "\n",
    "model.add(Conv2D(128,(3,3),activation='relu'))      #5th layer - Convolution layer with 128 neurons 3 x3 matrix to scan the image\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))            #6th layer - Maxpooling layer with 2 x 2 matrix to scan\n",
    "model.add(Dropout(0.25))                            #to avoid overfitting dropping 25% of neurons in each iteration\n",
    "\n",
    "model.add(Conv2D(256,(3,3),activation='relu'))      #7th layer - Convolution layer with 256 neurons 3 x3 matrix to scan the image\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))            #8th layer - Maxpooling layer with 2 x 2 matrix to scan\n",
    "model.add(Dropout(0.25))                            #to avoid overfitting dropping 25% of neurons in each iteration\n",
    "\n",
    "model.add(Conv2D(256,(3,3),activation='relu'))      #9th layer - Convolution layer with 256 neurons 3 x3 matrix to scan the image\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))            #10th layer - Maxpooling layer with 2 x 2 matrix to scan\n",
    "model.add(Dropout(0.25))                            #to avoid overfitting dropping 25% of neurons in each iteration\n",
    "\n",
    "model.add(Flatten())                        \n",
    "\n",
    "model.add(Dense(512, activation='relu'))            #11th Layer - Fully connected Dense layer with 512 neurons\n",
    "model.add(Dropout(0.5))                             #to avoid overfitting dropping 20% of the neurons in each iteration\n",
    "\n",
    "model.add(Dense(64, activation='relu'))             #12th Layer - Fully connected Dense layer with 64 neurons\n",
    "model.add(Dropout(0.25))                             #to avoid overfitting dropping 10% of the neurons in each iteration\n",
    "\n",
    "model.add(Dense(num_classes, activation='sigmoid')) #output layer\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_split=0.20)\n",
    "score=model.evaluate(X_test,Y_test,verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test Accuracy', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "275/275 [==============================] - 431s 2s/step - loss: 0.6933 - acc: 0.5439 - val_loss: 0.6796 - val_acc: 0.5227\n",
      "Epoch 2/30\n",
      "275/275 [==============================] - 432s 2s/step - loss: 0.4750 - acc: 0.7771 - val_loss: 0.3008 - val_acc: 0.8866\n",
      "Epoch 3/30\n",
      "275/275 [==============================] - 428s 2s/step - loss: 0.2822 - acc: 0.8968 - val_loss: 0.2686 - val_acc: 0.9064\n",
      "Epoch 4/30\n",
      "275/275 [==============================] - 431s 2s/step - loss: 0.2617 - acc: 0.9073 - val_loss: 0.2610 - val_acc: 0.9095\n",
      "Epoch 5/30\n",
      "275/275 [==============================] - 432s 2s/step - loss: 0.2494 - acc: 0.9150 - val_loss: 0.2691 - val_acc: 0.9052\n",
      "Epoch 6/30\n",
      "275/275 [==============================] - 431s 2s/step - loss: 0.2422 - acc: 0.9162 - val_loss: 0.2364 - val_acc: 0.9168\n",
      "Epoch 7/30\n",
      "275/275 [==============================] - 431s 2s/step - loss: 0.2422 - acc: 0.9161 - val_loss: 0.2430 - val_acc: 0.9190\n",
      "Epoch 8/30\n",
      "275/275 [==============================] - 430s 2s/step - loss: 0.2391 - acc: 0.9180 - val_loss: 0.2299 - val_acc: 0.9223\n",
      "Epoch 9/30\n",
      "275/275 [==============================] - 422s 2s/step - loss: 0.2318 - acc: 0.9193 - val_loss: 0.2242 - val_acc: 0.9216\n",
      "Epoch 10/30\n",
      "275/275 [==============================] - 428s 2s/step - loss: 0.2339 - acc: 0.9221 - val_loss: 0.2231 - val_acc: 0.9233\n",
      "Epoch 11/30\n",
      "275/275 [==============================] - 429s 2s/step - loss: 0.2272 - acc: 0.9225 - val_loss: 0.2271 - val_acc: 0.9237\n",
      "Epoch 12/30\n",
      "275/275 [==============================] - 428s 2s/step - loss: 0.2261 - acc: 0.9224 - val_loss: 0.2310 - val_acc: 0.9203\n",
      "Epoch 13/30\n",
      "275/275 [==============================] - 430s 2s/step - loss: 0.2231 - acc: 0.9241 - val_loss: 0.2206 - val_acc: 0.9247\n",
      "Epoch 14/30\n",
      "275/275 [==============================] - 427s 2s/step - loss: 0.2242 - acc: 0.9233 - val_loss: 0.2336 - val_acc: 0.9177\n",
      "Epoch 15/30\n",
      "275/275 [==============================] - 428s 2s/step - loss: 0.2143 - acc: 0.9270 - val_loss: 0.2147 - val_acc: 0.9259\n",
      "Epoch 16/30\n",
      "275/275 [==============================] - 428s 2s/step - loss: 0.2203 - acc: 0.9243 - val_loss: 0.2075 - val_acc: 0.9316\n",
      "Epoch 17/30\n",
      "275/275 [==============================] - 426s 2s/step - loss: 0.2153 - acc: 0.9252 - val_loss: 0.2143 - val_acc: 0.9281\n",
      "Epoch 18/30\n",
      "275/275 [==============================] - 423s 2s/step - loss: 0.2094 - acc: 0.9272 - val_loss: 0.2200 - val_acc: 0.9230\n",
      "Epoch 19/30\n",
      "275/275 [==============================] - 418s 2s/step - loss: 0.2160 - acc: 0.9265 - val_loss: 0.2140 - val_acc: 0.9249\n",
      "Epoch 20/30\n",
      "275/275 [==============================] - 425s 2s/step - loss: 0.2141 - acc: 0.9260 - val_loss: 0.2247 - val_acc: 0.9246\n",
      "Epoch 21/30\n",
      "275/275 [==============================] - 429s 2s/step - loss: 0.2200 - acc: 0.9242 - val_loss: 0.2094 - val_acc: 0.9253\n",
      "Epoch 22/30\n",
      "275/275 [==============================] - 426s 2s/step - loss: 0.2136 - acc: 0.9257 - val_loss: 0.2299 - val_acc: 0.9228\n",
      "Epoch 23/30\n",
      "275/275 [==============================] - 423s 2s/step - loss: 0.2088 - acc: 0.9272 - val_loss: 0.2204 - val_acc: 0.9257\n",
      "Epoch 24/30\n",
      "275/275 [==============================] - 426s 2s/step - loss: 0.2070 - acc: 0.9300 - val_loss: 0.2258 - val_acc: 0.9210\n",
      "Epoch 25/30\n",
      "275/275 [==============================] - 427s 2s/step - loss: 0.2108 - acc: 0.9280 - val_loss: 0.1966 - val_acc: 0.9330\n",
      "Epoch 26/30\n",
      "275/275 [==============================] - 427s 2s/step - loss: 0.2080 - acc: 0.9288 - val_loss: 0.2226 - val_acc: 0.9199\n",
      "Epoch 27/30\n",
      "275/275 [==============================] - 428s 2s/step - loss: 0.2091 - acc: 0.9290 - val_loss: 0.2098 - val_acc: 0.9293\n",
      "Epoch 28/30\n",
      "275/275 [==============================] - 421s 2s/step - loss: 0.2057 - acc: 0.9289 - val_loss: 0.2081 - val_acc: 0.9266\n",
      "Epoch 29/30\n",
      "275/275 [==============================] - 422s 2s/step - loss: 0.2057 - acc: 0.9296 - val_loss: 0.2090 - val_acc: 0.9296\n",
      "Epoch 30/30\n",
      "275/275 [==============================] - 427s 2s/step - loss: 0.2046 - acc: 0.9291 - val_loss: 0.2128 - val_acc: 0.9256\n",
      "Test loss: 0.147536630439\n",
      "Test Accuracy 0.950672238372\n"
     ]
    }
   ],
   "source": [
    "#Building the 3rd Deep Convolution Neural Network with image Augmentation\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size=64\n",
    "epochs=30\n",
    "Img_size=100\n",
    "num_classes=2\n",
    "input_shape=(Img_size, Img_size, 3)\n",
    "\n",
    "datagen=ImageDataGenerator(shear_range=0.2,\n",
    "                           zoom_range=0.2,\n",
    "                           width_shift_range = 0.2,\n",
    "                           height_shift_range = 0.2,\n",
    "                           fill_mode = 'nearest',\n",
    "                           rotation_range = 30,\n",
    "                           horizontal_flip=True,\n",
    "                           validation_split=0.2)\n",
    "\n",
    "train_generator=datagen.flow(X_train, Y_train, batch_size=batch_size, subset=\"training\",shuffle=True, seed=50)\n",
    "valid_generator=datagen.flow(X_train, Y_train, batch_size=batch_size, subset=\"validation\",shuffle=True, seed=50)\n",
    "\n",
    "                                \n",
    "test_datagen=ImageDataGenerator()\n",
    "test_generator=test_datagen.flow(X_test, Y_test,batch_size=batch_size, shuffle=False, seed=50)\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3),             #1st layer - Convolution layer with 32 neurons and 3 x 3 matrix to scan the image\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))   \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))            #2ndlayer - Maxpooling layer with 2 x 2 matrix to scan\n",
    "model.add(Dropout(0.25))                            #to avoid overfitting dropping 25% of neurons in each iteration\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))       #3rd layer - Convolution layer with 64 neurons 3 x3 matrix to scan the image\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))            #4th layer - Maxpooling layer with 2 x 2 matrix to scan\n",
    "model.add(Dropout(0.25))                            #to avoid overfitting dropping 25% of neurons in each iteration\n",
    "\n",
    "model.add(Conv2D(128,(3,3),activation='relu'))      #5th layer - Convolution layer with 128 neurons 3 x3 matrix to scan the image\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))            #6th layer - Maxpooling layer with 2 x 2 matrix to scan\n",
    "model.add(Dropout(0.25))                            #to avoid overfitting dropping 25% of neurons in each iteration\n",
    "\n",
    "model.add(Conv2D(256,(3,3),activation='relu'))      #7th layer - Convolution layer with 256 neurons 3 x3 matrix to scan the image\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))            #8th layer - Maxpooling layer with 2 x 2 matrix to scan\n",
    "model.add(Dropout(0.25))                            #to avoid overfitting dropping 25% of neurons in each iteration\n",
    "\n",
    "model.add(Flatten())                        \n",
    "\n",
    "model.add(Dense(512, activation='relu'))            #9th layer - Fully connected Dense layer with 512 neurons\n",
    "model.add(Dropout(0.5))                             #to avoid overfitting dropping 50% of the neurons in each iteration\n",
    "model.add(Dense(num_classes, activation='sigmoid')) #output layer\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "STEP_SIZE_TRAIN= train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID= valid_generator.n//valid_generator.batch_size\n",
    "STEP_SIZE_TEST= test_generator.n//test_generator.batch_size\n",
    "\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch = STEP_SIZE_TRAIN,\n",
    "                    validation_data = valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=epochs)\n",
    "\n",
    "score=model.evaluate_generator(generator=test_generator, steps=STEP_SIZE_TEST)\n",
    "print('Test loss:', score[0])\n",
    "print('Test Accuracy', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
